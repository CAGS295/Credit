{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibu/anaconda3/envs/tf36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ibu/anaconda3/envs/tf36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split as splitMe\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos finales con semilla fija; MSE para muestra de prueba; Histograma del error en muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Winton Stock Market Challenge\n",
    "\n",
    "https://www.kaggle.com/c/the-winton-stock-market-challenge/\n",
    "\n",
    "In this competition the challenge is to predict the return of a stock, given the history of the past few days. \n",
    "\n",
    "We provide 5-day windows of time, days D-2, D-1, D, D+1, and D+2. You are given returns in days D-2, D-1, and part of day D, and you are asked to predict the returns in the rest of day D, and in days D+1 and D+2.\n",
    "\n",
    "During day D, there is intraday return data, which are the returns at different points in the day. We provide 180 minutes of data, from t=1 to t=180. In the training set you are given the full 180 minutes, in the test set just the first 120 minutes are provided.\n",
    "\n",
    "For each 5-day window, we also provide 25 features, Feature_1 to Feature_25. These may or may not be useful in your prediction.\n",
    "\n",
    "Each row in the dataset is an arbitrary stock at an arbitrary 5 day time window.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer Modelo:\n",
    "Intentar precedir el rendimiento del día $D_{+1}$ utilizando las variables econonómicas y los rendimientos de $D_{-1}$ y $D_{-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos\n",
    "\n",
    "Se cargan los datos y se aplica una Normalizacíon Gaussiana al conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEXTRA_FEATURES = 25;\n",
    "nFEATURES = 121 -1 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trSet=pd.read_csv('./Data/Kaggle/train.csv',index_col=0);\n",
    "trSet.fillna(trSet.mean(),inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = (trSet.iloc[:,nEXTRA_FEATURES:nEXTRA_FEATURES+nFEATURES+1],trSet.iloc[:,nEXTRA_FEATURES+nFEATURES+60+1]);\n",
    "X, Y = (trSet.iloc[:,0:nEXTRA_FEATURES+2],trSet.iloc[:,nEXTRA_FEATURES+nFEATURES+60+1]);\n",
    "miuY = Y.mean()\n",
    "sigmaY = Y.std()\n",
    "Xnorm = (X - X.mean())/X.std();\n",
    "Ynorm = (Y - Y.mean())/Y.std();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnorm,Xnormt,Ynorm,Ynormt = splitMe(Xnorm,Ynorm, test_size=0.05, random_state=1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Exploration:\n",
    "Como parte de la exploración de variables intentaremos visualizar por pares las variables independientes y con un mapa de color la variable dependiente que se intenta predecir. En concreto, se intenta ver si hay alguna transformación que ayude a separar los datos linealmente con las variables pareadas. Creamos una pila $Stack$ donde están las combinaciones de columnas. 27**C**2 plots - la diagonal ex [1,1],[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pesado casi 27C2 plots!!!\n",
    "# a = (nEXTRA_FEATURES + 2 )//2\n",
    "# b = (nEXTRA_FEATURES + 2) - a\n",
    "# stack = np.stack(np.meshgrid(np.arange(a),np.arange(a+b)), -1).reshape(-1, 2)\n",
    "# for pair in stack:\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     ax = plt.gca()\n",
    "# #     Enfocar plot a una desviación estandar\n",
    "# #     ax.set_xlim([-1,1])\n",
    "# #     ax.set_ylim([-1,1])\n",
    "#     plt.scatter(Xnorm.iloc[:,pair[0]].values,Xnorm.iloc[:,pair[1]].values,c=Ynorm.values,cmap=cm.jet,alpha=.5)\n",
    "#     plt.title(\"{}  {}\".format(pair[0],pair[1]))\n",
    "#     plt.colorbar()    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = nFEATURES;\n",
    "M = 3#12;\n",
    "N = 9#10;\n",
    "d1units = 12;\n",
    "d2units = 12;\n",
    "d3units = 12;\n",
    "d1activation = tf.nn.sigmoid;\n",
    "d2activation = tf.nn.sigmoid;\n",
    "d3activation = tf.nn.sigmoid;\n",
    "droprate = .4;\n",
    "logitsunit = 1;\n",
    "LEARNING_RATE = 0.01;\n",
    "EPOCHS = 2000;\n",
    "BATCH_SIZE = 1000;\n",
    "n_batches = 1000;\n",
    "SEED=1;\n",
    "reg_scale = .9;\n",
    "decay = .96;\n",
    "decayiters = 100000;\n",
    "test_length = Ynormt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresSet = Xnorm.values\n",
    "labelsSet = Ynorm.values.reshape((-1,1))\n",
    "mode = 'train'\n",
    "\n",
    "featuresSetTest = Xnormt.values\n",
    "labelsSetTest = Ynormt.values.reshape((-1,1))\n",
    "\n",
    "\n",
    "    \n",
    "batch_size = tf.placeholder(tf.int64);\n",
    "x = tf.placeholder(tf.float32, shape=[None,featuresSet.size/len(featuresSet)]);\n",
    "y = tf.placeholder(tf.float32, shape=[None,labelsSet.size/len(labelsSet)])    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(1000,seed=SEED).batch(BATCH_SIZE).repeat() #Could shuffle()\n",
    "    \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()    \n",
    "    \n",
    "    \n",
    "#   \"\"\"Model function for NN.\"\"\"\n",
    "\n",
    "  # Input Layer\n",
    "input_layer = tf.reshape(features, [-1, M*N])\n",
    "    \n",
    "dense1 = tf.layers.dense(inputs=input_layer, units=d1units, activation=d1activation)\n",
    "    \n",
    "x1_norm = tf.layers.batch_normalization(dense1,center=True, scale=True,\n",
    "                                            training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "dense2 = tf.layers.dense(inputs=x1_norm, units=d2units, activation=d2activation)\n",
    "    \n",
    "x2_norm = tf.layers.batch_normalization(dense2,center=True, scale=True,\n",
    "                                            training=mode == tf.estimator.ModeKeys.TRAIN)    \n",
    "\n",
    "#     Dense Layer 3 \n",
    "dense3 = tf.layers.dense(inputs=x2_norm, units=d3units, activation=d3activation)\n",
    "dropout = tf.layers.dropout(inputs=dense3, rate=droprate, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "#     Output layer\n",
    "logits = tf.layers.dense(inputs=dropout, units=logitsunit)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels,logits)\n",
    "loss += tf.losses.get_regularization_loss()\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:        \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    global_step = tf.Variable(0)        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=\n",
    "                                        tf.train.exponential_decay(LEARNING_RATE,\n",
    "                                                                    global_step,\n",
    "                                                                    decayiters, decay, staircase=True))        \n",
    "    train_op = optimizer.minimize(loss=loss,global_step=global_step)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 24, Loss: 0.9024\n",
      "Epoch: 49, Loss: 0.8987\n",
      "Epoch: 74, Loss: 0.9017\n",
      "Epoch: 99, Loss: 0.8989\n",
      "Epoch: 124, Loss: 0.9003\n",
      "Epoch: 149, Loss: 0.8955\n",
      "Epoch: 174, Loss: 0.8967\n",
      "Epoch: 199, Loss: 0.8953\n",
      "Epoch: 224, Loss: 0.8964\n",
      "Epoch: 249, Loss: 0.8975\n",
      "Epoch: 274, Loss: 0.8972\n",
      "Epoch: 299, Loss: 0.8971\n",
      "Epoch: 324, Loss: 0.8948\n",
      "Epoch: 349, Loss: 0.9031\n",
      "Epoch: 374, Loss: 0.8947\n",
      "Epoch: 399, Loss: 0.8969\n",
      "Epoch: 424, Loss: 0.8953\n",
      "Epoch: 449, Loss: 0.8952\n",
      "Epoch: 474, Loss: 0.8951\n",
      "Epoch: 499, Loss: 0.8945\n",
      "Epoch: 524, Loss: 0.8967\n",
      "Epoch: 549, Loss: 0.8948\n",
      "Epoch: 574, Loss: 0.8969\n",
      "Epoch: 599, Loss: 0.8964\n",
      "Epoch: 624, Loss: 0.8942\n",
      "Epoch: 649, Loss: 0.8955\n",
      "Epoch: 674, Loss: 0.8955\n",
      "Epoch: 699, Loss: 0.8964\n",
      "Epoch: 724, Loss: 0.8942\n",
      "Epoch: 749, Loss: 0.8945\n",
      "Epoch: 774, Loss: 0.8942\n",
      "Epoch: 799, Loss: 0.8948\n",
      "Epoch: 824, Loss: 0.8958\n",
      "Epoch: 849, Loss: 0.8947\n",
      "Epoch: 874, Loss: 0.8934\n",
      "Epoch: 899, Loss: 0.8923\n",
      "Epoch: 924, Loss: 0.8914\n",
      "Epoch: 949, Loss: 0.8956\n",
      "Epoch: 974, Loss: 0.8938\n",
      "Epoch: 999, Loss: 0.8952\n",
      "Epoch: 1024, Loss: 0.8916\n",
      "Epoch: 1049, Loss: 0.8922\n",
      "Epoch: 1074, Loss: 0.8935\n",
      "Epoch: 1099, Loss: 0.8942\n",
      "Epoch: 1124, Loss: 0.8898\n",
      "Epoch: 1149, Loss: 0.8906\n",
      "Epoch: 1174, Loss: 0.8924\n",
      "Epoch: 1199, Loss: 0.8914\n",
      "Epoch: 1224, Loss: 0.8910\n",
      "Epoch: 1249, Loss: 0.8928\n",
      "Epoch: 1274, Loss: 0.8910\n",
      "Epoch: 1299, Loss: 0.8917\n",
      "Epoch: 1324, Loss: 0.8910\n",
      "Epoch: 1349, Loss: 0.8898\n",
      "Epoch: 1374, Loss: 0.8920\n",
      "Epoch: 1399, Loss: 0.8901\n",
      "Epoch: 1424, Loss: 0.8910\n",
      "Epoch: 1449, Loss: 0.8899\n",
      "Epoch: 1474, Loss: 0.8913\n",
      "Epoch: 1499, Loss: 0.8905\n",
      "Epoch: 1524, Loss: 0.8906\n",
      "Epoch: 1549, Loss: 0.8901\n",
      "Epoch: 1574, Loss: 0.8899\n",
      "Epoch: 1599, Loss: 0.8899\n",
      "Epoch: 1624, Loss: 0.8901\n",
      "Epoch: 1649, Loss: 0.8895\n",
      "Epoch: 1674, Loss: 0.8901\n",
      "Epoch: 1699, Loss: 0.8912\n",
      "Epoch: 1724, Loss: 0.8896\n",
      "Epoch: 1749, Loss: 0.8881\n",
      "Epoch: 1774, Loss: 0.8889\n",
      "Epoch: 1799, Loss: 0.8898\n",
      "Epoch: 1824, Loss: 0.8879\n",
      "Epoch: 1849, Loss: 0.8880\n",
      "Epoch: 1874, Loss: 0.8888\n",
      "Epoch: 1899, Loss: 0.8897\n",
      "Epoch: 1924, Loss: 0.8882\n",
      "Epoch: 1949, Loss: 0.8892\n",
      "Epoch: 1974, Loss: 0.8891\n",
      "Epoch: 1999, Loss: 0.8883\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            # initialise iterator with train data TODO\n",
    "    sess.run(iterator.initializer, feed_dict={ x: featuresSet, y: labelsSet, batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):                    \n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value                    \n",
    "        if (i+1) % 25 == 0:\n",
    "            print(\"Epoch: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    print(\"Training complete\")\n",
    "#             return sess\n",
    "    # initialise iterator with test data\n",
    "    mode = 'eval'\n",
    "    sess.run(iterator.initializer, feed_dict={ x: featuresSetTest, y: labelsSetTest, batch_size: test_length})\n",
    "    Yh,Y1,_,L = sess.run([logits,labels,train_op, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test Loss: 1.2985'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test Loss: {:.4f}\".format(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enframe(Yh,Y,miu,sigma):\n",
    "    Yhu=Yh*sigma+miu\n",
    "    Yu=Y1*sigma+miu\n",
    "\n",
    "    a1= pd.DataFrame(Yhu)\n",
    "    a1.columns = ['yhat']\n",
    "    a2= pd.DataFrame(Yu)\n",
    "    a2.columns = ['y']    \n",
    "    return a1.join(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008140712380409241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = enframe(Yh,Y1,miuY,sigmaY)\n",
    "E = aux.iloc[:,0].values - aux.iloc[:,1].values\n",
    "SE = E@E\n",
    "MSE = SE/len(E)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribución del error para 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4580bcc470>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f459e8a6d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(aux.iloc[:,0]-aux.iloc[:,1]).hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2: Red Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = (trSet.iloc[:,nEXTRA_FEATURES:nEXTRA_FEATURES+nFEATURES+1],trSet.iloc[:,nEXTRA_FEATURES+nFEATURES+60+1]);\n",
    "miuY = Y.mean()\n",
    "sigmaY = Y.std()\n",
    "Xnorm = (X - X.mean())/X.std();\n",
    "Ynorm = (Y - miuY)/sigmaY;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnorm,Xnormt,Ynorm,Ynormt = splitMe(Xnorm,Ynorm, test_size=0.05, random_state=1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X.columns.size;\n",
    "M = 1;\n",
    "N = INPUT_SIZE;\n",
    "c1filter = 32;\n",
    "c2filter = 16;\n",
    "c1kernel = [1,3];\n",
    "c2kernel = [1,2];\n",
    "c1padding = 'same';\n",
    "c2padding = 'same';\n",
    "c1activation = None;\n",
    "c2activation = None;\n",
    "d1units = 8;\n",
    "d1activation = tf.nn.sigmoid;\n",
    "droprate = .4;\n",
    "logitsunit = 1;\n",
    "LEARNING_RATE = 0.1;\n",
    "EPOCHS = 1000;\n",
    "BATCH_SIZE = 200;\n",
    "n_batches = 200;\n",
    "SEED=2;\n",
    "reg_scale = .9;\n",
    "decay = .96;\n",
    "decayiters = 100000;\n",
    "test_length = Ynormt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "featuresSet = Xnorm.values\n",
    "labelsSet = Ynorm.values.reshape((-1,1))\n",
    "mode = 'train'\n",
    "\n",
    "featuresSetTest = Xnormt.values\n",
    "labelsSetTest = Ynormt.values.reshape((-1,1))\n",
    "    \n",
    "batch_size = tf.placeholder(tf.int64);\n",
    "x = tf.placeholder(tf.float32, shape=[None,featuresSet.size/len(featuresSet)]);\n",
    "y = tf.placeholder(tf.float32, shape=[None,labelsSet.size/len(labelsSet)])    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(1000,seed=SEED).batch(BATCH_SIZE).repeat()\n",
    "    \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()    \n",
    "    \n",
    "    \n",
    "#   \"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "  # Input Layer\n",
    "input_layer = tf.reshape(features, [-1, M*N,1, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs = input_layer,\n",
    "    filters = c1filter,\n",
    "    kernel_size = c1kernel,\n",
    "    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=reg_scale),\n",
    "    padding =c1padding,\n",
    "    activation = c1activation)\n",
    "    \n",
    "x1_norm = tf.layers.batch_normalization(conv1,center=True, scale=True,\n",
    "                                        training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs = x1_norm,\n",
    "    filters = c2filter,\n",
    "    kernel_size = c2kernel,\n",
    "    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=reg_scale),\n",
    "    padding = c2padding,\n",
    "    activation = c2activation)\n",
    "    \n",
    "x2_norm = tf.layers.batch_normalization(conv2,center=True, scale=True,\n",
    "                                        training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  # Dense Layer\n",
    "pool2_flat = tf.reshape(x2_norm, [-1, M * N * c2filter])\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=d1units, activation=d1activation)\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=droprate, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "#     Output layer\n",
    "logits = tf.layers.dense(inputs=dropout, units=logitsunit)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels,logits)\n",
    "loss += tf.losses.get_regularization_loss()\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:        \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    global_step = tf.Variable(0)        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=\n",
    "                                        tf.train.exponential_decay(LEARNING_RATE,\n",
    "                                                                    global_step,\n",
    "                                                                    decayiters, decay, staircase=True))        \n",
    "    train_op = optimizer.minimize(loss=loss,global_step=global_step)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 24, Loss: 0.9985\n",
      "Epoch: 49, Loss: 0.9991\n",
      "Epoch: 74, Loss: 1.0097\n",
      "Epoch: 99, Loss: 1.0037\n",
      "Epoch: 124, Loss: 0.9975\n",
      "Epoch: 149, Loss: 1.0013\n",
      "Epoch: 174, Loss: 1.0070\n",
      "Epoch: 199, Loss: 1.0031\n",
      "Epoch: 224, Loss: 1.0018\n",
      "Epoch: 249, Loss: 1.0165\n",
      "Epoch: 274, Loss: 1.0064\n",
      "Epoch: 299, Loss: 1.0190\n",
      "Epoch: 324, Loss: 1.0018\n",
      "Epoch: 349, Loss: 0.9978\n",
      "Epoch: 374, Loss: 1.0014\n",
      "Epoch: 399, Loss: 1.0012\n",
      "Epoch: 424, Loss: 1.0083\n",
      "Epoch: 449, Loss: 1.0021\n",
      "Epoch: 474, Loss: 1.0077\n",
      "Epoch: 499, Loss: 1.0007\n",
      "Epoch: 524, Loss: 1.0093\n",
      "Epoch: 549, Loss: 1.0045\n",
      "Epoch: 574, Loss: 0.9991\n",
      "Epoch: 599, Loss: 1.0038\n",
      "Epoch: 624, Loss: 1.0184\n",
      "Epoch: 649, Loss: 1.0033\n",
      "Epoch: 674, Loss: 1.0019\n",
      "Epoch: 699, Loss: 1.0081\n",
      "Epoch: 724, Loss: 1.0273\n",
      "Epoch: 749, Loss: 0.9965\n",
      "Epoch: 774, Loss: 1.0057\n",
      "Epoch: 799, Loss: 1.0057\n",
      "Epoch: 824, Loss: 1.0270\n",
      "Epoch: 849, Loss: 1.0004\n",
      "Epoch: 874, Loss: 1.0024\n",
      "Epoch: 899, Loss: 1.0083\n",
      "Epoch: 924, Loss: 1.0044\n",
      "Epoch: 949, Loss: 1.0030\n",
      "Epoch: 974, Loss: 0.9980\n",
      "Epoch: 999, Loss: 1.0004\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    " with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # initialise iterator with train data TODO\n",
    "        sess.run(iterator.initializer, feed_dict={ x: featuresSet, y: labelsSet, batch_size: BATCH_SIZE})\n",
    "        print('Training...')\n",
    "        for i in range(EPOCHS):\n",
    "            tot_loss = 0\n",
    "            for _ in range(n_batches):                    \n",
    "                _, loss_value = sess.run([train_op, loss])\n",
    "                tot_loss += loss_value                    \n",
    "            if (i+1) % 25 == 0:\n",
    "                print(\"Epoch: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "        print(\"Training complete\")\n",
    "    # initialise iterator with test data\n",
    "        mode = 'eval'\n",
    "        sess.run(iterator.initializer, feed_dict={ x: featuresSetTest, y: labelsSetTest, batch_size: test_length})\n",
    "        Yh,Y1,_,L = sess.run([logits,labels,train_op, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test Loss: 0.8189'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Test Loss: {:.4f}\".format(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005124114453792572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = enframe(Yh,Y1,miuY,sigmaY)\n",
    "E = aux.iloc[:,0].values - aux.iloc[:,1].values\n",
    "SE = E@E\n",
    "MSE = SE/len(E)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribución del error para 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4578402400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f457843e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(aux.iloc[:,0]-aux.iloc[:,1]).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
